name: word_tokenize
channels:
  - defaults
dependencies:
  - _libgcc_mutex=0.1=main
  - ca-certificates=2019.10.16=0
  - certifi=2019.9.11=py36_0
  - libedit=3.1.20181209=hc058e9b_0
  - libffi=3.2.1=hd88cf55_4
  - libgcc-ng=9.1.0=hdf63c60_0
  - libstdcxx-ng=9.1.0=hdf63c60_0
  - ncurses=6.1=he6710b0_1
  - openssl=1.1.1d=h7b6447c_3
  - pip=19.3.1=py36_0
  - python=3.6.9=h265db76_0
  - readline=7.0=h7b6447c_5
  - setuptools=42.0.1=py36_0
  - sqlite=3.30.1=h7b6447c_0
  - tk=8.6.8=hbc83047_0
  - wheel=0.33.6=py36_0
  - xz=5.2.4=h14c3975_4
  - zlib=1.2.11=h7b6447c_3
  - pip:
    - args==0.1.0
    - chardet==3.0.4
    - click==7.0
    - clint==0.5.1
    - idna==2.8
    - joblib==0.13.2
    - languageflow==1.1.13a1
    - nltk==3.4.5
    - numpy==1.17.4
    - python-crfsuite==0.9.6
    - requests==2.22.0
    - scikit-learn==0.20.3
    - scipy==1.3.3
    - six==1.11.0
    - tabulate==0.8.6
    - tqdm==4.39.0
    - underthesea==1.1.17a1
    - urllib3==1.25.7
    - vncorenlp==1.0.3
prefix: /home/neil-do/anaconda3/envs/word_tokenize

